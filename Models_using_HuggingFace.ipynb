{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgzuGbwFhInmBZBM8v1IgK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
        "\n",
        "This notebook can run on a low-cost or free T4 runtime.\n",
        "\n",
        "## Please note\n",
        "\n",
        "I've added some new material in the middle of this lab to get more intuition on what a Transformer actually is. Later in the course, when we fine-tune LLMs, you'll get a deeper understanding of this."
      ],
      "metadata": {
        "id": "gaug_Nex4VEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the neccessary libraries"
      ],
      "metadata": {
        "id": "auIi-Dvc5XDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --force-reinstall torch==2.6.0"
      ],
      "metadata": {
        "id": "_vHlZBatnRE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes --upgrade"
      ],
      "metadata": {
        "id": "mquXmx8Bmt2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OklUcXkh4II4"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the neccessary libraries"
      ],
      "metadata": {
        "id": "jc8DVtbU5fAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "FH-1rPW14nOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code tells that the HuggingFace tool to authenticate you with that key, and also writes it into your Git settings so you can clone or push to private Hugging Face repos without typing your password again"
      ],
      "metadata": {
        "id": "opIHw_pX5yyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "kjZgYq2t5oDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line assigns a friendly variable name to the exact Hugging Face model identifier you’ll use when loading the pipeline."
      ],
      "metadata": {
        "id": "nOlPDYbG6PL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\"\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
      ],
      "metadata": {
        "id": "lOhVNyOv5oeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a **list of message objects** you’ll feed into a chat model"
      ],
      "metadata": {
        "id": "-R7KoKn26RVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "pPXhyFf86SJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization Configuration - This allows us to load the model into memory and use less memory"
      ],
      "metadata": {
        "id": "D4qxB9BY6qQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "DyECFQr46qmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Prepares text for a language model (LLAMA) by converting it into a numerical format the model can understand. It involves loading a tokenizer, defining padding, and applying a chat template to format the input text."
      ],
      "metadata": {
        "id": "gdjHb2Ly7WOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "6GgHDbLG7J2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLMA"
      ],
      "metadata": {
        "id": "jOpK2423o2dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantization Configuration:**\n",
        "\n",
        "This allows us to load the model into memory and use less memory"
      ],
      "metadata": {
        "id": "GeILIPuzo6tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ],
      "metadata": {
        "id": "zXWA8WXp8WQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory: ,.1f} MB\")"
      ],
      "metadata": {
        "id": "LTUz3wP2pmsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "9UBqfOWHp1Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(inputs, max_new_tokens=80)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "56drNTmap3vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for Data Scientists\"}\n",
        "  ]\n",
        "generate(GEMMA2, messages)"
      ],
      "metadata": {
        "id": "_IHwGaIgsi9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
        "  memory = model.get_memory_footprint() / 1e6\n",
        "  print(f\"Memory footprint: {memory:,.1f} MB\")"
      ],
      "metadata": {
        "id": "cfN0moO5szGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to quantize and load the model back to Hugging Face"
      ],
      "metadata": {
        "id": "Kg16enL7uLEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"google/gemma-2-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "model.push_to_hub(\"korarishi1027/rishi-2-2b-it\")\n",
        "tokenizer.push_to_hub(\"korarishi1027/rishi-2-2b-it\")"
      ],
      "metadata": {
        "id": "yJ4Q2QPYrIzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}